









# 大模型部署

### 部署场景

- 服务器端：cpu部署、GPU部署、多卡集群(如何实现分布式推理)
- 移动端：手机、移动机器人（如何量化优化、算力需求）

### 大模型部署面临的挑战

1. 计算量大

![image-20240424201452883](https://s2.loli.net/2024/04/24/Kd4Qxpsw3uJqRi5.png)

2. 内存开销

   LLM在推理过程中为避免重复计算，会将计算注意力的考的KV进行缓存

![image-20240424201620907](https://s2.loli.net/2024/04/24/SsyZ3zgv49bnuN5.png)

3. 访存瓶颈

   大模型推理是"访存密集"型任务。硬件计算速度很快，但是显存带宽不足

4. 动态请求

   请求量不确定（同一时间可能多人发起请求）、请求时间不确定、Token逐个生成，生成数量不确定

### 大模型部署方法

1. `模型剪枝`：移除模型中的部分参数或部分层，

   ![image-20240424202936057](https://s2.loli.net/2024/04/24/7lEKRfLbWSwJm9c.png)

2. `知识蒸馏`：从存在冗余参数中，推断出模型有潜力通过更少的参数量来达到同等或者更好的效果。先训练一个大模型，为Teacher网络，再去让Teacher网络训练一个参数量少的的Student网络，从而降低训练难度。

   ![image-20240424203034378](https://s2.loli.net/2024/04/24/QDsxjzCGeTM8bEa.png)

3. `量化`：以参数或计算中间结果精度下降换空间节省（以及同时带来的性能提升）

   - 计算密集（compute-bound）: 指推理过程中，绝大部分时间消耗在数值计算上；针对计算密集型场景，可以通过使用更快的硬件计算单元来提升计算速。
   - 访存密集（memory-bound）: 指推理过程中，绝大部分时间消耗在数据读取上；针对访存密集型场景，一般通过减少访存次数、提高计算访存比或降低访存量来优化。
   
   常见的 LLM 模型由于 Decoder Only 架构的特性，实际推理时大多数的时间都消耗在了逐 Token 生成阶段（Decoding 阶段），是典型的访存密集型场景。
   
   优化 LLM 模型推理中的访存密集问题，可以使用**KV8量化**和**W4A16**量化。
   
   - KV8量化是指将逐 Token（Decoding）生成过程中的上下文 K 和 V 中间结果进行 INT8 量化（计算时再反量化），以降低生成过程中的显存占用。
   - W4A16 量化，将 FP16 的模型权重量化为 INT4，Kernel 计算时，访存量直接降为 FP16 模型的 1/4，大幅降低了访存成本。Weight Only 是指仅量化权重，数值计算依然采用 FP16（需要将 INT4 权重反量化）
   
   提升推理速度的原因：大模型推理时是一种访存密集型任务，访存的性能瓶颈是远大于计算的瓶颈。通过量化，可以将访存量降低，比如原来是32位，量化为16位，访存量降低为原来的1/2，从而降低数据传输的时间，提升运算效率
   
   ![image-20240424203127323](https://s2.loli.net/2024/04/24/ho8VmgBR65ZcYWf.png)

KV Cache是一种缓存技术，通过存储键值对的形式来复用计算结果，以达到提高性能和降低内存消耗的目的。在大规模训练和推理中，KV Cache可以显著减少重复计算量，从而提升模型的推理速度。

模型在运行时，**占用的显存可大致分为三部分**：

- 模型参数本身占用的显存、
- KV Cache占用的显存
-  以及中间运算结果占用的显存。

理想情况下，KV Cache全部存储于显存，以加快访存速度。当显存空间不足时，也可以将KV Cache放在内存，通过缓存管理器控制将当前需要使用的数据放入显存。

## 基础作业

完成以下任务，并将实现过程记录截图：

- 配置 LMDeploy 运行环境

  ```bash
  studio-conda -t lmdeploy -o pytorch-2.1.2
  conda activate lmdeploy
  pip install lmdeploy[all]==0.3.0
  ```

  

  ![image-20240424213246714](https://s2.loli.net/2024/04/24/zeGgfc8KDNPJHW2.png)

- 以命令行方式与 InternLM2-Chat-1.8B 模型对话

  使用Transformer库运行模型

  ```python
  import torch
  from transformers import AutoTokenizer, AutoModelForCausalLM
  
  tokenizer = AutoTokenizer.from_pretrained("/root/internlm2-chat-1_8b", trust_remote_code=True)
  
  # Set `torch_dtype=torch.float16` to load model in float16, otherwise it will be loaded as float32 and cause OOM Error.
  model = AutoModelForCausalLM.from_pretrained("/root/internlm2-chat-1_8b", torch_dtype=torch.float16, trust_remote_code=True).cuda()
  model = model.eval()
  
  inp = "hello"
  print("[INPUT]", inp)
  response, history = model.chat(tokenizer, inp, history=[])
  print("[OUTPUT]", response)
  
  inp = "please provide three suggestions about time management"
  print("[INPUT]", inp)
  response, history = model.chat(tokenizer, inp, history=history)
  print("[OUTPUT]", response)
  ```

  ![image-20240425091151830](https://s2.loli.net/2024/04/25/5LY2SixwlF9ZtMU.png)

  使用LMDeploy与模型对话

  ```bash
  #使用LMDeploy与模型进行对话的通用命令格式为：
  #lmdeploy chat [HF格式模型路径/TurboMind格式模型路径]
  lmdeploy chat /root/internlm2-chat-1_8b
  ```

  ![image-20240425092101297](https://s2.loli.net/2024/04/25/ZpJtRMAo1BGxqun.png)

## 进阶作业

- 设置KV Cache最大占用比例为0.4，开启W4A16量化，以命令行方式与模型对话。

  W4A16即为权重4bit，激活值16bit， Weight Only 量化。Weight Only 是指仅量化权重，数值计算依然采用 FP16（需要将 INT4 权重反量化）

  LMDeploy的KV Cache管理器可以通过设置`--cache-max-entry-count`参数，控制KV缓存**占用剩余显存的最大比例**。默认的比例为0.8。

  ![image-20240425095614709](https://s2.loli.net/2024/04/25/gi4aWRns6qmo3L1.png)

  此时显存占用为20936MB。下面，改变`--cache-max-entry-count`参数，设为0.4。

  ```bash
  lmdeploy chat /root/internlm2-chat-1_8b --cache-max-entry-count 0.4
  ```

  ![image-20240425100311411](https://s2.loli.net/2024/04/25/N6WPFBjwckrfLVg.png)

  显存占用变为12784MB

  <details>
      <summary>AWQ算法详情</summary>
      AWQ（Adaptive Window Query）算法是一种用于数据流管理系统的查询优化技术。它通过动态调整查询窗口的大小来适应数据流的变化，从而优化处理性能和资源利用率。该算法考虑到数据的到达率和查询的复杂性，自适应地调整窗口大小，以达到预设的处理目标，如延迟和吞吐量的最优化。
  </details>

  设置KV Cache最大占用比例为0.4，开启W4A16量化

  ![image-20240425102841600](https://s2.loli.net/2024/04/25/GvmOaLqFr4phQcx.png)

  ![image-20240425102335946](https://s2.loli.net/2024/04/25/MfqvcUeY6Elb9hJ.png)

  显存占用变为11492MB

  对话结果如下：

  ![image-20240425102929980](https://s2.loli.net/2024/04/25/qS1IbX97QJjLnON.png)

- 以API Server方式启动 lmdeploy，开启 W4A16量化，调整KV Cache的占用比例为0.4，分别使用命令行客户端与Gradio网页客户端与模型对话。

  在生产环境下，我们有时会将大模型封装为API接口服务，供客户端访问。

  ![image-20240425103257471](https://s2.loli.net/2024/04/25/VpFHn3ey8OEqhJs.png)

  我们把从架构上把整个服务流程分成下面几个模块。

  - 模型推理/服务。主要提供模型本身的推理，一般来说可以和具体业务解耦，专注模型推理本身性能的优化。可以以模块、API等多种方式提供。
  - API Server。中间协议层，把后端推理/服务通过HTTP，gRPC或其他形式的接口，供前端调用。
  - Client。可以理解为前端，与用户交互的地方。通过通过网页端/命令行去调用API接口，获取模型推理/服务。

  值得说明的是，以上的划分是一个相对完整的模型，但在实际中这并不是绝对的。比如可以把“模型推理”和“API Server”合并，有的甚至是三个流程打包在一起提供服务。

  ```bash
  lmdeploy serve api_server \
  # model-format： 使用模型格式（awq：量化格式，hf：huggingface未量化格式）；
  # quant-policy：kv量化开关，需要和前面的量化保持一致（0表示不使用kv量化，4表示开启kv量化）；
  # server-name：API服务器的服务；
  # server-port：API服务端口；
  # tp：表示参数并行数量（GPU数量）
      /root/internlm2-chat-1_8b-4bit \
      --model-format awq \ # 注意使用的是用awq方法量化后的模型
      --quant-policy 4 \
      --server-name 0.0.0.0 \
      --server-port 23333 \
      --tp 1
  ```

  ![image-20240425105843161](https://s2.loli.net/2024/04/25/zedTGSxqQR9mgKU.png)

  客户端对话

  ```bash
  lmdeploy serve api_client http://localhost:23333
  ```

  ![image-20240425110532521](https://s2.loli.net/2024/04/25/YGROK1UMlpfhyb9.png)

  ![image-20240425110649979](https://s2.loli.net/2024/04/25/kXPGbr3s2NJCWMp.png)

  Gradio网页客户端对话

  ```bash
  lmdeploy serve gradio http://localhost:23333 \
      --server-name 0.0.0.0 \
      --server-port 6006
  # 本地端口映射
  # ssh -CNg -L 6006:127.0.0.1:6006 root@ssh.intern-ai.org.cn -p <你的ssh端口号>
  ```

  ![image-20240425111957728](https://s2.loli.net/2024/04/25/16pskAmU9fxGKE3.png)

  ![image-20240425111846799](https://s2.loli.net/2024/04/25/DJg2rOwyHuA69s4.png)

- 使用W4A16量化，调整KV Cache的占用比例为0.4，使用Python代码集成的方式运行internlm2-chat-1.8b-4bit模型。

  ```python
  from lmdeploy import pipeline, TurbomindEngineConfig
  
  # 调低 k/v cache内存占比调整为总显存的 40%
  backend_config = TurbomindEngineConfig(cache_max_entry_count=0.4, model_format='awq')
  
  pipe = pipeline('/root/internlm2-chat-1_8b-4bit',
                  backend_config=backend_config)
  response = pipe(['Hi, pls intro yourself', '上海是'])
  print(response)
  ```

  ![image-20240425112646504](https://s2.loli.net/2024/04/25/noIMe9a71UEHxWv.png)

- 使用 LMDeploy 运行视觉多模态大模型 llava gradio demo。

  ```python
  import gradio as gr
  from lmdeploy import pipeline, TurbomindEngineConfig
  
  
  backend_config = TurbomindEngineConfig(session_len=8192) # 图片分辨率较高时请调高session_len
  # pipe = pipeline('liuhaotian/llava-v1.6-vicuna-7b', backend_config=backend_config) 非开发机运行此命令
  pipe = pipeline('/share/new_models/liuhaotian/llava-v1.6-vicuna-7b', backend_config=backend_config)
  
  def model(image, text):
      if image is None:
          return [(text, "请上传一张图片。")]
      else:
          response = pipe((text, image)).text
          return [(text, response)]
  
  demo = gr.Interface(fn=model, inputs=[gr.Image(type="pil"), gr.Textbox()], outputs=gr.Chatbot())
  demo.launch()   
  ```

  ![image-20240425123358641](https://s2.loli.net/2024/04/25/vIDOBfjomytFh5W.png)

- 将 LMDeploy Web Demo 部署到 [OpenXLab](https://github.com/InternLM/Tutorial/blob/camp2/tools/openxlab-deploy) (TODO)