# 书生·浦语大模型全链路开源开放体系

专用模型：正对特定任务，一个模型解决一个问题 eg：人脸识别、语音处理

通用大模型：一个模型应对多种任务、多种模态 eg：GPT-4

## 书生·浦语大模型开源历程

![image-20240406190115728](https://s2.loli.net/2024/04/06/etBx8MzwdWHC3Rs.png)

### InternLM2的改进

1. 在数据集上进行了优化：更丰富的语料库，针对性的数据补齐
2. 工具调用能力增强（计算器、代码解释器等）

### 模型到应用典型流程

![image-20240406191158985](https://s2.loli.net/2024/04/06/gZ4H3hbMoKql6LP.png)

## 书生·浦语体系

![image-20240406191330009](https://s2.loli.net/2024/04/06/rn4lQRYtx8LusBE.png)

1. #### 数据[OpenDataLab](https://opendatalab.com/) eg:万卷1.0、万卷cc

2. #### 预训练框架

   ![image-20240406191801022](https://s2.loli.net/2024/04/06/dUz6erkhF1A3VOS.png)

3. #### 微调

   迁移学习中使用的一种技术，它利用预训练的机器学习模型作为新任务的起点，然后在新任务特定的数据集上进一步对模型进行训练。微调可以提高模型在新任务上的性能，同时减少训练所需的数据和计算资源的数量。

   - 增量续训：让baseline学习一些新知识，如垂直领域知识

     训练数据：文章、书籍、代码等

   - 有监督微调（SFT）：让模型学会理解各种指令进行对话，或注入少量领域知识
     训练数据：高质量对话、问答数据
     - 有监督微调包含全参数微调和部分参数微调
   - 微调框架：XTuner
     ![image-20240406192437044](https://s2.loli.net/2024/04/06/NgImScLeR9PWGvq.png)

4. #### 评测[OpenCompass](https://opencompass.org.cn/)

5. #### 部署LMDeploy

   ![image-20240406192939520](https://s2.loli.net/2024/04/06/AKZJca7pnPf9roY.png)

6. 智能体Lagent

   在LLM语境下，Agent可以理解为在某种能自主理解、规划决策、执行复杂任务的智能体

   **Agent = LLM+Planning+Feedback+Tool use** 

# InternLM2技术报告

## 摘要

> 大型语言模型（LLMs）如ChatGPT和GPT-4的进化引发了关于人工通用智能（AGI）来临的讨论。然而，在开源模型中复制这样的进步一直是个挑战。本文介绍了InternLM2，一种开源的LLM，它在6个维度和30个基准测试、长上下文建模和开放式主观评估中的全面评估上超越了其前辈，通过创新的预训练和优化技术。InternLM2的预训练过程被详细描述，强调了包括文本、代码和长上下文数据在内的多样数据类型的准备。InternLM2有效地捕捉长期依赖关系，在预训练和微调阶段最初在4k令牌上进行训练，之后进展到32k令牌，展现出在200k“大海捞针”测试中的显著性能。InternLM2进一步通过监督式微调（SFT）和一种新颖的基于人类反馈的条件在线强化学习（COOL RLHF）策略进行校准，该策略解决了人类偏好的冲突和奖励黑客问题。通过发布不同训练阶段和模型大小的InternLM2模型，我们为社区提供了模型进化的洞察。

## 一、介绍

LLM发展主要阶段：预训练-->有监督微调(SFT,Supervised Fine-Tuning )-->人类反馈强化学习(RLHF,Reinforcement Learning from Human Feedback)

`预训练`:主要使用大量分成了万亿token的自然文本语料，使大模型具有广泛的知识库和基本能力

数据的质量是预训练阶段最关键的因素（怎样预处理）

`扩展LLM上下文长度`

> 如何有效地扩展LLM的上下文长度是目前的一个研究热点，因为许多下游应用，例如检索增强生成（RAG）（Gao et al., 2023）和代理（Xi et al., 2023），都依赖于在长上下文中。 InternLM2 首先采用组查询注意力 (GQA)，以在推断长序列时实现更小的内存占用。在预训练阶段，我们首先使用 4k 上下文文本训练 InternLM2，然后将训练语料库转换为高质量的 32k 文本进行进一步训练。完成后，通过位置编码外推（LocalLLaMA，2023），InternLM2 在 20 万上下文中的“大海捞针”测试中取得了值得称赞的性能

## 二、基础配置

`预训练框架`：InternEvo

### 模型架构

选择遵循 LLaMA 的结构设计原则，Grouped-Query Attention (GQA)

## 三、预训练

### 文本数据

将网页、论文、专利、书籍转为指定格式，按语言和类别进行分类，按JSON格式存储，对数据进行清洗（基于规则过滤、删除重复数据、安全过滤、质量过滤）

![image-20240407135848162](https://s2.loli.net/2024/04/07/Zzfb6BdEkWTixhj.png)

## 四、对齐

### SFT

使用包含 1000 万条指令数据实例的数据集，将数据样本转换为 ChatML 格式

### RHLF

提出了Conditional OnLine RLHF (COOL RLHF).引入了条件奖励机制来协调不同的偏好，该机制允许奖励模型根据特定条件动态地将注意力分配给各种偏好，从而优化地整合多种偏好。采用多轮Online RLHF策略，使LLM能够及时适应新的人类反馈
